\documentclass[a4paper]{article}
\usepackage{ctex} 
\usepackage[affil-it]{authblk}
\usepackage[backend=bibtex,style=numeric]{biblatex}
\usepackage{amsmath}

\usepackage{geometry}
\geometry{margin=1.5cm, vmargin={0pt,1cm}}
\setlength{\topmargin}{-1cm}
\setlength{\paperheight}{29.7cm}
\setlength{\textheight}{25.3cm}

\addbibresource{citation.bib}

\begin{document}
% =================================================
\title{Numerical Analysis homework week1}

\author{谭希 3220100027
  \thanks{Electronic address: \texttt{3220100027@zju.edu.cn}}}
\affil{(数学与应用数学2201), Zhejiang University }


\date{Due time: \today}

\maketitle

% ============================================
\section*{I. Consider the bisection method starting with the initial interval [1.5, 3.5]. In the following questions “the
interval” refers to the bisection interval whose width
changes across different loops.}

\subsection*{I-a What is the width of the interval at the $n$th step?}  
  
The initial interval is $[1.5, 3.5]$, and its width is:  
\[b - a = 3.5 - 1.5 = 2\]  
  
At each iteration, the width is halved. Therefore, at the $n$th step, the interval width is:  
\[\text{d} = \frac{2}{2^{n-1}}\]  

\subsection*{I-b What is the supremum of the distance between
the root r and the midpoint of the interval?}

 At the \( n \)-th step, the current interval width is \(\frac{2}{2^{n-1}}\), and the midpoint is located at the center of this interval.

The root \( r \) can be at any position within the interval, and the farthest distance from the midpoint is half of the current interval width. If the current interval width is \(\frac{2}{2^{n-1}}\), the maximum distance is:\[\text{$d_n$} = \frac{1}{2} \times \frac{2}{2^n} = \frac{1}{2^n}\]

Therefore, at the \( n \)-th step, the supremum of the distance between the root \( r \) and the midpoint of the interval is \(\frac{1}{2^n}\).

\section*{II. In using the bisection algorithm with its initial interval as [$a_0$, $b_0$] with $a_0 > 0$ , we want to determine the root with its relative error no greater than.Prove that this goal of accuracy is guaranteed by the following choice of the number of steps,\[n \geq \frac{\log(b_0 - a_0) - \log \epsilon - \log a_0}{\log 2} - 1.\]}

To ensure that the relative error of the root is no greater than \( \epsilon \) using the bisection method with an initial interval \([a_0, b_0]\) where \( a_0 > 0 \), we need to choose the number of steps \( n \) appropriately. 

The length of the interval after \( n \) steps is:
\[\frac{b_0 - a_0}{2^n}\]

We want this length to be at most \( \epsilon \cdot a_0 \), so ：
\[\frac{b_0 - a_0}{2^n} \leq \epsilon \cdot a_0\]

Taking the logarithm of both sides:
\[\log \left( \frac{b_0 - a_0}{2^n} \right) \leq \log(\epsilon \cdot a_0)\]

\[\log (b_0 - a_0) - n \log 2 \leq \log \epsilon + \log a_0\]

\[-n \log 2 \leq \log \epsilon + \log a_0 - \log (b_0 - a_0)\]

\[n \geq \frac{\log (b_0 - a_0) - \log \epsilon - \log a_0}{\log 2}\]

steps must be integers, the final number of steps \( n \) should be:

\[n \geq \frac{\log (b_0 - a_0) - \log \epsilon - \log a_0}{\log 2} - 1\]
This ensures that the relative error of the root is no greater than \( \epsilon \).

\section*{III. Perform four iterations of Newton’s method for the polynomial equation $p(x) = 4x^3 - 2x^2 + 3 = 0$ with the starting point $x_0 = -1$. Use a hand calculator and organize results of the iterations in a table.}  

Calculate the function  \( p(x) \) and its derivative \( p'(x) \)，then use Newton's method iteration formula:
\[ x_{n+1} = x_n - \frac{p(x_n)}{p'(x_n)} \]

\[ p(x) = 4x^3 - 2x^2 + 3 \]
\[ p'(x) = 12x^2 - 4x \]

First Iteration:
\(x_0 = -1\)
\[p(-1) = 4(-1)^3 - 2(-1)^2 + 3 = -4 - 2 + 3 = -3\]\[p'(-1) = 12(-1)^2 - 4(-1) = 12 + 4 = 16\]
   
\[x_1 = -1 - \frac{-3}{16} = -1 + \frac{3}{16} = -\frac{13}{16} \approx -0.8125\]

Second Iteration:
\( x_1 = -0.8125 \)

\[p(-0.8125) = 4(-0.8125)^3 - 2(-0.8125)^2 + 3\]

so:

\[p(-0.8125) = -2.14453125 - 1.3125 + 3 = -0.4658\]

\[x_2 = -0.8125 - \frac{-0.45703125}{11.125} = -0.8125 + 0.041 \approx -0.7708\]

Similarly, other values can be calculated iteratively.
Below is the table summarizing the results:

\begin{table}[h!]
  \centering
  \begin{tabular}{|c|c|c|c|}
    \hline
    \textbf{Iteration}& \( x_n \) & \( p(x_n) \) & \(p'(x_n)\)\\
    \hline
    0 & -1.0000 & -3.0000 & 16\\
    \hline
    1 & -0.8125 & -0.4658 & 11.1719\\
    \hline
    2 & -0.7708 & -0.0201 &10.2129\\
    \hline
    3 & -0.7688 & -4.3708e-05 &10.1686\\
    \hline
    4 & -0.7688 & -2.0741e-10 &10.1685\\
    \hline
  \end{tabular}
\end{table}

\section*{IV. Consider a variation of Newton’s method in which only the derivative at $x_0$ is used,\[x_{n+1} = x_n - \frac{f(x_n)}{f'(x_0)}.\] Find C and s such that $e_{n+1} = C e_n^s$，where en is the error of Newton’s method at step n, s is a constant, and C may depend on  \(x_n\), the true solution $\alpha$, and the derivative of the function f.}

   Let the error at step \( n \) be defined as \( e_n = x_n - \alpha \), where \( \alpha \) is the true root of \( f(x) = 0 \). 
   
   Using the modified Newton’s method formula, we get:
   \[x_{n+1} = x_n - \frac{f(x_n)}{f'(x_0)}.\]

    \[x_{n+1} - \alpha = x_n - \alpha - \frac{f'(\xi_n) (x_n - \alpha)} {f'(x_0)}\]
    \[x_{n+1} - \alpha = (1 - \frac{f'(\xi_n)}{f'(x_0)}(x_n - \alpha)\]


\[ e_{n+1} \approx (1-\frac{f'(x_n)}{f'(x_0)})e_n\]
Thus:
\[s = 1, C =  1-\frac{f'(\alpha)}{f'(x_0)}\]

\section*{V. Within $\left(-\frac{\pi}{2}, \frac{\pi}{2}\right)$, will the iteration $x_{n+1} = \tan^{-1}x_n$ converge?}

Within the interval \(\left(-\frac{\pi}{2}, \frac{\pi}{2}\right)\), \(f(x)\) is continuous and monotonically increasing.

We need to analyze the convergence of the iteration \(x_{n+1} = f(x_n)\). To check for convergence, we can find the fixed point of the function, which satisfies the equation:

\[x = \tan^{-1} x\]

Let \(x^* = \tan^{-1} x^*\). By analyzing this equation, we find that the solution is \(x^* = 0\), since \(\tan^{-1}(0) = 0\).

Next, we analyze the convergence of the iteration. According to the fixed-point convergence theorem, if \(f\) is a contraction mapping near the fixed point, the iterative sequence will converge.

We compute \(f'(x)\) to check its behavior near the fixed point \(x^* = 0\):

\[f'(x) = \frac{1}{1+x^2}\]

Within the interval \(\left(-\frac{\pi}{2}, \frac{\pi}{2}\right)\), \(f'(x)\) is always positive and less than 1. Specifically, we have:

\[f'(0) = 1\]

To ensure that \(f\) is a contraction near the fixed point, we note that:

\[f'(x) < 1 \quad \text{for } x \neq 0\]

Since \(f\) is monotonically increasing and the slope near the fixed point is less than 1, we conclude that the iteration \(x_{n+1} = \tan^{-1} x_n\) will converge to the fixed point \(x^* = 0\).

The iteration \(x_{n+1} = \tan^{-1} x_n\) converges within the interval \(\left(-\frac{\pi}{2}, \frac{\pi}{2}\right)\).

\section*{VI. Let \( p > 1 \). What is the value of the following continued fraction?
\[x = \frac{1}{p + \frac{1}{p + \frac{1}{p + \cdots}}}\]
Prove that the sequence of values converges. (Hint: this can be interpreted as \( x = \lim_{n \to \infty} x_n \), where \( x_1 = \frac{1}{p} \), \( x_2 = \frac{1}{p + \frac{1}{p}} \), \( x_3 = \frac{1}{p + \frac{1}{p + \frac{1}{p}}} \), and so forth. Formulate \( x \) as a fixed point of some function.)}

\[x_1 = \frac{1}{p}, \quad x_2 = \frac{1}{p + x_1}, \quad x_3 = \frac{1}{p + x_2}, \quad \ldots\]
express the general term \( x_n \) as:
\[x_n = \frac{1}{p + x_{n-1}}\]

Assuming the limit \[x = \lim_{n \to \infty} x_n\]exists, substituting \( x \) into the recurrence gives:
\[x = \frac{1}{p + x}\]

multiply both sides by \( p + x \):
\[x(p + x) = 1\]
\[px + x^2 = 1\]
\[x^2 + px - 1 = 0\]

then:
\[x = \frac{-p \pm \sqrt{p^2 + 4}}{2}\]
Since \( p > 1 \):
\[x = \frac{-p + \sqrt{p^2 + 4}}{2}\]

Next, we need to prove that the sequence \( \{x_n\} \) converges.

\textbf{Boundedness:}

To proof \( x_n > 0 \). By induction:
- For \( n = 1 \), \( x_1 = \frac{1}{p} > 0 \) since \( p > 1 \).
- Suppose \( x_n > 0 \). Then \( x_{n+1} = \frac{1}{p + x_n} > 0 \) because \( p + x_n > p > 0 \).

Therefore, by induction, \( x_n > 0 \) for all \( n \).

Next, we show that \( x_n < \frac{1}{p - 1} \):
- For \( n = 1 \):
\[x_1 = \frac{1}{p} < \frac{1}{p - 1} \quad \text{(since \( p > 1 \))}\]
- Assume \( x_n < \frac{1}{p - 1} \). Then:
\[x_{n+1} = \frac{1}{p + x_n} > \frac{1}{p + \frac{1}{p - 1}} = \frac{1}{\frac{p(p - 1) + 1}{p - 1}} = \frac{p - 1}{p^2 - p + 1}
\]
because \( p^2 - 2p + 1 > 0 \),\( \frac{1}{p + \frac{1}{p - 1}} < \frac{1}{p - 1} \). This is true for \( p > 1 \).

Thus, \( x_n \) is bounded above by \( \frac{1}{p - 1} \).

\textbf{Monotonicity:}

To prove that \( x_n \) is decreasing, we show \( x_{n+1} < x_n \):
\[\frac{1}{p + x_n} < x_n \implies 1 < x_n(p + x_n) \implies 1 < px_n + x_n^2\]This will hold true if we can show that \( x_n^2 + px_n - 1 > 0 \).

Given the roots of \( x^2 + px - 1 = 0 \) are \( \frac{-p \pm \sqrt{p^2 + 4}}{2} \) and knowing that \( x_n > 0 \) for sufficiently large \( n \), the quadratic \( x^2 + px - 1 \) is positive for \( x > \frac{-p + \sqrt{p^2 + 4}}{2} \), which is valid for large \( n \).

Thus, \( x_n \) is decreasing and bounded below by \( 0 \). Therefore, by the Monotone Convergence Theorem, the sequence \( \{x_n\} \) converges.

In conclusion, the value of the continued fraction is:
\[x = \frac{-p + \sqrt{p^2 + 4}}{2}\]

\section*{VII. What happens in problem II if \( a_0 < 0 < b_0 \)? Derive an inequality of the number of steps similar to that in II. In this case, is the relative error still an appropriate measure? }

zaizhelimianbuyeshiyiyangzidem
keyia najiuanzhaonisou shoude 

According to Problem II: \[\frac{b_n - a_n}{2^n} \leq \epsilon|r|\]
\[n \geq \frac{\log(b_0 - a_0) - \log \epsilon - \log a_0}{\log 2} - 1.\]
If the root is \(0\) or so close to \(0\), the relative error may become too large or undefined\\
so, the relative error isn’t an appropriate measure,absolute error is better.

\section*{VIII. (*) Consider solving \( f(x) = 0 \) (where \( f \in C^{k+1} \)) by Newton's method with the starting point \( x_0 \) close to a root of multiplicity \( k \). Note that \( \alpha \) is a zero of multiplicity \( k \) of the function \( f \) if and only if \( f^{(k)}(\alpha) \neq 0 \) and \( f^{(i)}(\alpha) = 0 \) for all \( i < k \).}

\subsection*{VIII-a How can a multiple zero be detected by examining the behavior of the points $(x_n, f(x_n))$?}

The standard Newton's iteration is \[x_{n+1} = x_n - \frac{f(x_n)}{f'(x_n)}\]
As \(x_n\) approaches to a multiple zero, \(f(x_n)\) may be always near to zero for several iterations and the iterates \(x_n\) remain very close together.

\subsection*{VIII-b Prove that if r is a zero of multiplicity k of the function f, then quadratic convergence in Newton’s iteration will be restored by making this modification:}
\[x_{n+1} = x_n - \frac{f(x_n)}{f'(x_n)}\]

In the case of a multiple root, can modify it to:
\[x_{n+1} = x_n - \frac{k f(x_n)}{f^{(k)}(x_n)}\]

Since \( r \) is a multiple root of \( k \), we can represent \( f(x) \) near \( r \) as:

\[f(x) = (x - r)^k g(x)\]

where \( g(r) \neq 0 \) (since \( f^{(k)}(r) \neq 0 \)).

\[f'(x) = k(x - r)^{k-1} g(x) + (x - r)^k g'(x)\]

substitute \( x_n \) close to \( r \):

\[f(x_n) = (x_n - r)^k g(x_n)\]
and
\[f'(x_n) \approx k(x_n - r)^{k - 1} g(r)\]
when \( x_n \) is close to \( r \), and \( g(x_n) \approx g(r) \).
So:
\[x_{n+1} = x_n - \frac{k f(x_n)}{f^{(k)}(x_n)} = x_n - \frac{k (x_n - r)^k g(x_n)}{g(r)}\]

Suppose \( x_n \) is close enough to \( r \):
\[x_{n+1} - r = x_n - r - \frac{k (x_n - r)^k g(r)}{g(r)} 
= x_n - r - (x_n - r)^k\]

As \( n \) becomes large enough, \( (x_n - r)^k \) will become very small. The quadratic term will yield:
\[|x_{n+1} - r| \approx C |x_n - r|^2\]

where \( C \) is a constant, thus confirming that \( x_n \) converges to \( r \) at a quadratic rate.

% ===============================================

\end{document}